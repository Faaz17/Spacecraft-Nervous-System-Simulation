"""
=============================================================================
DATA LOADER MODULE - Load Real Sensor Data from Hardware Simulation
=============================================================================

This module loads real sensor data generated by the hardware simulation team
(Vanshika & Varenya) from CSV files. It provides a unified interface to load
data from either teammate's simulation.

DATA SOURCES:
-------------
1. Vanshika (Steps 1-2): Basic sensor output
   - Classical sensor with noise and drift
   - Quantum sensor with precision readings

2. Varenya (Steps 1-4): Complete pipeline
   - Steps 1-2: Raw sensor output
   - Step 3: Environmental disturbances added
   - Step 4: ADC (Analog-to-Digital Conversion)

Author: Spacecraft Nervous System Simulation Team
Course: Quantum-IoT Hybrid Network for Spacecraft Decision Systems
=============================================================================
"""

import numpy as np
import pandas as pd
from pathlib import Path
from typing import Tuple, Optional
from dataclasses import dataclass


@dataclass
class SensorData:
    """
    Container for loaded sensor data.
    
    Attributes
    ----------
    time : np.ndarray
        Time axis in seconds
    ground_truth : np.ndarray
        The actual physical signal
    classical : np.ndarray
        Classical sensor readings
    quantum : np.ndarray
        Quantum sensor readings
    sample_rate : float
        Calculated sample rate in Hz
    duration : float
        Total duration in seconds
    source : str
        Description of the data source
    """
    time: np.ndarray
    ground_truth: np.ndarray
    classical: np.ndarray
    quantum: np.ndarray
    sample_rate: float
    duration: float
    source: str


def load_vanshika_data(
    filepath: str = "Quantum IoT Network Simulation Vanshika/sensor_raw_data_steps_1_2.csv"
) -> SensorData:
    """
    Load sensor data from Vanshika's simulation (Steps 1-2).
    
    Parameters
    ----------
    filepath : str
        Path to Vanshika's CSV file
    
    Returns
    -------
    SensorData
        Loaded and validated sensor data
    """
    print(f"[DATA_LOADER] Loading Vanshika's data from: {filepath}")
    
    # Load CSV file
    df = pd.read_csv(filepath)
    
    # Extract columns
    time = df['Time_s'].values
    ground_truth = df['Ground_Truth'].values
    classical = df['Classical_Output'].values
    quantum = df['Quantum_Output'].values
    
    # Calculate sample rate from time differences
    dt = np.mean(np.diff(time))
    sample_rate = 1.0 / dt
    duration = time[-1] - time[0]
    
    print(f"[DATA_LOADER] Loaded {len(time)} samples")
    print(f"[DATA_LOADER] Sample rate: {sample_rate:.1f} Hz")
    print(f"[DATA_LOADER] Duration: {duration:.3f} seconds")
    
    return SensorData(
        time=time,
        ground_truth=ground_truth,
        classical=classical,
        quantum=quantum,
        sample_rate=sample_rate,
        duration=duration,
        source="Vanshika - Steps 1-2 (Raw Sensors)"
    )


def load_varenya_data(
    filepath: str = "Quantum IoT Network Simulation Varenya/sensor_raw_data_steps_1_4_complete.csv",
    use_stage: str = "raw"
) -> SensorData:
    """
    Load sensor data from Varenya's simulation (Steps 1-4).
    
    Parameters
    ----------
    filepath : str
        Path to Varenya's CSV file
    use_stage : str
        Which processing stage to use:
        - "raw": Steps 1-2 raw sensor output
        - "env": Step 3 with environmental disturbances
        - "adc": Step 4 after ADC conversion
    
    Returns
    -------
    SensorData
        Loaded and validated sensor data
    """
    print(f"[DATA_LOADER] Loading Varenya's data from: {filepath}")
    print(f"[DATA_LOADER] Using stage: {use_stage}")
    
    # Load CSV file
    df = pd.read_csv(filepath)
    
    # Extract time and ground truth (always the same)
    time = df['Time_s'].values
    ground_truth = df['Ground_Truth'].values
    
    # Select columns based on requested stage
    if use_stage == "raw":
        classical = df['Classical_Sensor_Steps_1_2'].values
        quantum = df['Quantum_Sensor_Steps_1_2'].values
        source = "Varenya - Steps 1-2 (Raw Sensors)"
    elif use_stage == "env":
        classical = df['Classical_w_Env_Step_3'].values
        quantum = df['Quantum_w_Env_Step_3'].values
        source = "Varenya - Step 3 (Environmental Disturbances)"
    elif use_stage == "adc":
        classical = df['Classical_ADC_SAR_Step_4'].values
        quantum = df['Quantum_ADC_DS_Step_4'].values
        source = "Varenya - Step 4 (ADC Conversion)"
    else:
        raise ValueError(f"Unknown stage: {use_stage}. Use 'raw', 'env', or 'adc'")
    
    # Calculate sample rate
    dt = np.mean(np.diff(time))
    sample_rate = 1.0 / dt
    duration = time[-1] - time[0]
    
    print(f"[DATA_LOADER] Loaded {len(time)} samples")
    print(f"[DATA_LOADER] Sample rate: {sample_rate:.1f} Hz")
    print(f"[DATA_LOADER] Duration: {duration:.3f} seconds")
    
    return SensorData(
        time=time,
        ground_truth=ground_truth,
        classical=classical,
        quantum=quantum,
        sample_rate=sample_rate,
        duration=duration,
        source=source
    )


def load_performance_summary(
    filepath: str = "Quantum IoT Network Simulation Varenya/sensor_performance_summary.csv"
) -> pd.DataFrame:
    """
    Load the performance summary metrics from hardware simulation.
    
    Returns a DataFrame with SNR, RMS Error, and noise statistics.
    """
    print(f"[DATA_LOADER] Loading performance summary from: {filepath}")
    return pd.read_csv(filepath)


def detect_radiation_spikes(
    quantum: np.ndarray,
    ground_truth: np.ndarray,
    threshold_factor: float = 5.0
) -> np.ndarray:
    """
    Detect radiation spikes in quantum sensor data.
    
    Uses deviation from ground truth to identify anomalous readings.
    
    Parameters
    ----------
    quantum : np.ndarray
        Quantum sensor readings
    ground_truth : np.ndarray
        Ground truth signal
    threshold_factor : float
        Number of standard deviations for spike detection
    
    Returns
    -------
    spike_indices : np.ndarray
        Indices where spikes were detected
    """
    # Calculate deviation from ground truth
    deviation = np.abs(quantum - ground_truth)
    
    # Use robust statistics (median and MAD) for threshold
    median_dev = np.median(deviation)
    mad = np.median(np.abs(deviation - median_dev))  # Median Absolute Deviation
    
    # Threshold based on MAD (more robust than std)
    threshold = median_dev + threshold_factor * mad * 1.4826  # Scale factor for normal dist
    
    spike_indices = np.where(deviation > threshold)[0]
    
    print(f"[DATA_LOADER] Detected {len(spike_indices)} potential radiation spikes")
    
    return spike_indices


# =============================================================================
# MODULE TEST
# =============================================================================
if __name__ == "__main__":
    # Test loading both datasets
    print("\n" + "="*60)
    print("Testing Vanshika's Data")
    print("="*60)
    try:
        vanshika_data = load_vanshika_data()
        print(f"Classical range: [{vanshika_data.classical.min():.2f}, {vanshika_data.classical.max():.2f}]")
        print(f"Quantum range: [{vanshika_data.quantum.min():.2f}, {vanshika_data.quantum.max():.2f}]")
    except FileNotFoundError as e:
        print(f"File not found: {e}")
    
    print("\n" + "="*60)
    print("Testing Varenya's Data")
    print("="*60)
    try:
        for stage in ["raw", "env", "adc"]:
            print(f"\n--- Stage: {stage} ---")
            varenya_data = load_varenya_data(use_stage=stage)
            print(f"Classical range: [{varenya_data.classical.min():.2f}, {varenya_data.classical.max():.2f}]")
            print(f"Quantum range: [{varenya_data.quantum.min():.2f}, {varenya_data.quantum.max():.2f}]")
    except FileNotFoundError as e:
        print(f"File not found: {e}")

